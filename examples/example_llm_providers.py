#!/usr/bin/env python3
"""
Example Usage of Multiple LLM Providers
Crisis Management Multi-Agent System

This demonstrates how to use different LLM providers:
- Claude (Anthropic)
- OpenAI (GPT models)
- LM Studio (Local models)

Requirements:
    - For Claude: ANTHROPIC_API_KEY environment variable
    - For OpenAI: OPENAI_API_KEY environment variable
    - For LM Studio: Local LM Studio server running on http://localhost:1234
"""

import os
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from llm_integration.claude_client import ClaudeClient


def check_api_keys():
    """Check which API keys are available."""
    print("="*80)
    print("API KEY STATUS CHECK")
    print("="*80 + "\n")

    claude_key = os.getenv('ANTHROPIC_API_KEY')
    openai_key = os.getenv('OPENAI_API_KEY')

    print(f"Claude (Anthropic): {'‚úì Available' if claude_key else '‚úó Not set'}")
    print(f"OpenAI:             {'‚úì Available' if openai_key else '‚úó Not set'}")
    print(f"LM Studio:          ‚Ñπ Requires local server at http://localhost:1234")
    print()

    return {
        'claude': bool(claude_key),
        'openai': bool(openai_key),
        'lm_studio': True  # Always available if server is running
    }


def example_claude():
    """Example using Claude API."""
    print("="*80)
    print("EXAMPLE 1: Claude (Anthropic)")
    print("="*80 + "\n")

    api_key = os.getenv('ANTHROPIC_API_KEY')
    if not api_key:
        print("‚ö† ANTHROPIC_API_KEY not set. Skipping Claude example.")
        print("   To use: export ANTHROPIC_API_KEY='your-api-key-here'\n")
        return

    try:
        # Initialize Claude client
        client = ClaudeClient(api_key=api_key)
        print(f"‚úì Claude client initialized")
        print(f"  Model: {client.model}")
        print(f"  Max tokens: {client.max_tokens}")
        print(f"  Temperature: {client.temperature}\n")

        # Create a simple assessment prompt
        prompt = """You are a crisis management expert evaluating response options.

SCENARIO: Urban flood risk requiring immediate decision

ALTERNATIVES:
- A1: Immediate Mass Evacuation
- A2: Deploy Barriers + Selective Evacuation
- A3: Shelter-in-Place with Monitoring

Provide a brief assessment (2-3 sentences) focusing on safety priorities."""

        print("üìù Sending assessment request to Claude...")

        # Note: Actual API call would go here
        # response = client.get_completion(prompt)

        print("\n‚úì Example setup complete!")
        print("  (Actual API call commented out to avoid charges)")
        print("\nüí° To make actual API calls:")
        print("   1. Uncomment the response line above")
        print("   2. Add response parsing logic")
        print("   3. Run with valid API key\n")

    except Exception as e:
        print(f"‚úó Error with Claude: {e}\n")


def example_openai():
    """Example using OpenAI API."""
    print("="*80)
    print("EXAMPLE 2: OpenAI (GPT Models)")
    print("="*80 + "\n")

    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("‚ö† OPENAI_API_KEY not set. Skipping OpenAI example.")
        print("   To use: export OPENAI_API_KEY='your-api-key-here'\n")
        return

    try:
        # Example OpenAI setup (pseudocode - requires openai package)
        print("‚úì OpenAI client setup")
        print("  Model: gpt-4 or gpt-3.5-turbo")
        print("  Max tokens: 1024")
        print("  Temperature: 0.7\n")

        print("üìù Example OpenAI implementation:")
        print("""
from openai import OpenAI

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a crisis management expert."},
        {"role": "user", "content": "Assess flood response options..."}
    ],
    max_tokens=1024,
    temperature=0.7
)

assessment = response.choices[0].message.content
""")

        print("\nüí° To use OpenAI:")
        print("   1. Install: pip install openai")
        print("   2. Set API key: export OPENAI_API_KEY='sk-...'")
        print("   3. Implement the code above\n")

    except Exception as e:
        print(f"‚úó Error with OpenAI: {e}\n")


def example_lm_studio():
    """Example using LM Studio (local models)."""
    print("="*80)
    print("EXAMPLE 3: LM Studio (Local Models)")
    print("="*80 + "\n")

    print("‚Ñπ LM Studio allows running models locally with OpenAI-compatible API\n")

    print("üìã Setup Instructions:")
    print("   1. Download LM Studio from https://lmstudio.ai/")
    print("   2. Download a model (e.g., Mistral, Llama 2, Phi-2)")
    print("   3. Start local server (default: http://localhost:1234)")
    print("   4. Use OpenAI-compatible endpoint\n")

    print("üìù Example LM Studio implementation:")
    print("""
from openai import OpenAI

# Point to LM Studio local server
client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="not-needed"  # LM Studio doesn't require API key
)

response = client.chat.completions.create(
    model="local-model",  # Use model loaded in LM Studio
    messages=[
        {"role": "system", "content": "You are a crisis management expert."},
        {"role": "user", "content": "Assess flood response options..."}
    ],
    max_tokens=1024,
    temperature=0.7
)

assessment = response.choices[0].message.content
""")

    print("\n‚úÖ Benefits of LM Studio:")
    print("   ‚Ä¢ No API costs - runs locally")
    print("   ‚Ä¢ Data privacy - no external calls")
    print("   ‚Ä¢ Offline capability")
    print("   ‚Ä¢ OpenAI-compatible API")
    print("   ‚Ä¢ Easy model switching\n")

    print("üí° Recommended models for Crisis MAS:")
    print("   ‚Ä¢ Mistral-7B (balanced performance)")
    print("   ‚Ä¢ Llama-2-13B (better reasoning)")
    print("   ‚Ä¢ Phi-2 (lightweight, fast)")
    print("   ‚Ä¢ OpenHermes-2.5 (instruction-following)\n")


def example_provider_comparison():
    """Compare different LLM providers."""
    print("="*80)
    print("PROVIDER COMPARISON")
    print("="*80 + "\n")

    print("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
    print("‚îÇ Provider   ‚îÇ Cost     ‚îÇ Speed     ‚îÇ Quality   ‚îÇ Privacy    ‚îÇ")
    print("‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
    print("‚îÇ Claude     ‚îÇ $$       ‚îÇ Fast      ‚îÇ Excellent ‚îÇ External   ‚îÇ")
    print("‚îÇ OpenAI     ‚îÇ $$       ‚îÇ Fast      ‚îÇ Excellent ‚îÇ External   ‚îÇ")
    print("‚îÇ LM Studio  ‚îÇ Free     ‚îÇ Medium    ‚îÇ Good      ‚îÇ Local      ‚îÇ")
    print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")
    print()

    print("üìä When to use each:")
    print()
    print("Claude (Anthropic):")
    print("  ‚úì Production deployments needing best quality")
    print("  ‚úì Complex reasoning and analysis tasks")
    print("  ‚úì When cost is not primary concern")
    print("  ‚úó Requires API key and internet")
    print()

    print("OpenAI (GPT-4/3.5):")
    print("  ‚úì Production deployments with good ecosystem")
    print("  ‚úì Wide range of model options (GPT-4, GPT-3.5)")
    print("  ‚úì Mature API with extensive documentation")
    print("  ‚úó Requires API key and internet")
    print()

    print("LM Studio (Local):")
    print("  ‚úì Development and testing without API costs")
    print("  ‚úì Privacy-sensitive scenarios (data stays local)")
    print("  ‚úì Offline deployment requirements")
    print("  ‚úì Educational/research use")
    print("  ‚úó Requires local compute resources (GPU recommended)")
    print("  ‚úó Generally lower quality than Claude/GPT-4")
    print()


def example_multi_provider_strategy():
    """Example of using multiple providers with fallback."""
    print("="*80)
    print("MULTI-PROVIDER STRATEGY WITH FALLBACK")
    print("="*80 + "\n")

    print("üìã Implementation Strategy:")
    print()
    print("1. Primary Provider (Best Quality)")
    print("   ‚îî‚îÄ> Try Claude or GPT-4 first")
    print()
    print("2. Fallback Provider (Cost-Effective)")
    print("   ‚îî‚îÄ> If primary fails, try GPT-3.5 or local model")
    print()
    print("3. Local Provider (Always Available)")
    print("   ‚îî‚îÄ> LM Studio as last resort")
    print()

    print("üìù Example Implementation:")
    print("""
def get_llm_assessment(prompt, providers=['claude', 'openai', 'lm_studio']):
    '''Try multiple providers with fallback.'''

    for provider in providers:
        try:
            if provider == 'claude' and os.getenv('ANTHROPIC_API_KEY'):
                return get_claude_assessment(prompt)

            elif provider == 'openai' and os.getenv('OPENAI_API_KEY'):
                return get_openai_assessment(prompt)

            elif provider == 'lm_studio':
                # Always try local as last resort
                return get_lm_studio_assessment(prompt)

        except Exception as e:
            print(f"Provider {provider} failed: {e}")
            continue

    raise RuntimeError("All providers failed")

# Usage
assessment = get_llm_assessment(
    prompt="Assess crisis response options...",
    providers=['claude', 'openai', 'lm_studio']  # Priority order
)
""")
    print()

    print("‚úÖ Benefits:")
    print("   ‚Ä¢ Reliability: Automatic fallback if primary fails")
    print("   ‚Ä¢ Cost optimization: Use cheaper providers when possible")
    print("   ‚Ä¢ Flexibility: Easy to add/remove providers")
    print("   ‚Ä¢ Offline capability: LM Studio as final fallback\n")


def example_crisis_scenario():
    """Full crisis scenario example with multiple providers."""
    print("="*80)
    print("COMPLETE CRISIS SCENARIO EXAMPLE")
    print("="*80 + "\n")

    print("üö® SCENARIO: Urban Flood Crisis Decision")
    print()
    print("Context:")
    print("  ‚Ä¢ Location: Metro area, 50,000 residents")
    print("  ‚Ä¢ Threat: 200mm rainfall in 48h")
    print("  ‚Ä¢ Time: 4 hours to decide")
    print("  ‚Ä¢ Stakes: Lives, infrastructure, economy")
    print()

    print("üéØ Decision Alternatives:")
    print()
    print("A1: Immediate Mass Evacuation")
    print("    ‚Ä¢ Safety: Highest")
    print("    ‚Ä¢ Cost: $5M")
    print("    ‚Ä¢ Time: 4h response")
    print("    ‚Ä¢ Risk: Panic, traffic congestion")
    print()

    print("A2: Deploy Barriers + Selective Evacuation")
    print("    ‚Ä¢ Safety: Moderate-High")
    print("    ‚Ä¢ Cost: $2M")
    print("    ‚Ä¢ Time: 3h response")
    print("    ‚Ä¢ Risk: Barrier failure")
    print()

    print("A3: Shelter-in-Place + Monitoring")
    print("    ‚Ä¢ Safety: Moderate")
    print("    ‚Ä¢ Cost: $500K")
    print("    ‚Ä¢ Time: 1h response")
    print("    ‚Ä¢ Risk: Rapid deterioration")
    print()

    print("üë• Expert Perspectives Needed:")
    print("   ‚Ä¢ Meteorologist: Weather prediction")
    print("   ‚Ä¢ Structural Engineer: Infrastructure capacity")
    print("   ‚Ä¢ Emergency Manager: Response coordination")
    print("   ‚Ä¢ Economist: Cost-benefit analysis")
    print()

    print("üîÑ LLM Provider Usage Strategy:")
    print()
    print("Phase 1 - Initial Analysis (Claude/GPT-4):")
    print("   ‚Ä¢ Use best model for critical initial assessment")
    print("   ‚Ä¢ Get detailed reasoning and risk analysis")
    print("   ‚Ä¢ Establish baseline expert opinions")
    print()

    print("Phase 2 - Alternative Perspectives (GPT-3.5/Local):")
    print("   ‚Ä¢ Generate diverse expert viewpoints")
    print("   ‚Ä¢ Cost-effective for multiple iterations")
    print("   ‚Ä¢ Capture range of professional opinions")
    print()

    print("Phase 3 - Consensus Building (Mixed):")
    print("   ‚Ä¢ Use Claude for final synthesis")
    print("   ‚Ä¢ Local models for sensitivity analysis")
    print("   ‚Ä¢ Validate consensus across providers")
    print()


def main():
    """Main example runner."""
    print("\n" + "="*80)
    print("MULTI-PROVIDER LLM EXAMPLES FOR CRISIS MAS")
    print("="*80 + "\n")

    # Check available providers
    available = check_api_keys()

    # Run examples
    if available['claude']:
        example_claude()

    if available['openai']:
        example_openai()

    example_lm_studio()
    example_provider_comparison()
    example_multi_provider_strategy()
    example_crisis_scenario()

    print("="*80)
    print("SUMMARY")
    print("="*80 + "\n")
    print("‚úì Demonstrated three LLM provider options")
    print("‚úì Showed fallback strategy for reliability")
    print("‚úì Explained when to use each provider")
    print("‚úì Provided complete implementation examples")
    print()
    print("Next Steps:")
    print("  1. Choose provider(s) based on your needs")
    print("  2. Set up API keys or LM Studio")
    print("  3. Integrate into Crisis MAS agents")
    print("  4. Test with real crisis scenarios")
    print()
    print("For more details, see:")
    print("  ‚Ä¢ llm_integration/README.md - LLM integration docs")
    print("  ‚Ä¢ examples/example_multi_llm_providers.py - Original multi-provider example")
    print("  ‚Ä¢ examples/README.md - All examples overview")
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    main()
