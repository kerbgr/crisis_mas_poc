"""
Extracted from LLM Training Methodology.
Auto-generated by extract_code_from_methodology.py
"""

def equal_opportunity(true_labels_a, pred_labels_a, true_labels_b, pred_labels_b):
    """
    Test if true positive rate is similar across groups.

    Example:
        - Group A (Athens): Model gets 90% of answers correct
        - Group B (Rural): Model gets 65% of answers correct
        - Equal opportunity ratio: 0.72 (BIASED - model worse for rural)
    """
    import numpy as np

    # True positive rate (recall) for each group
    tpr_a = np.sum((true_labels_a == 1) & (pred_labels_a == 1)) / np.sum(true_labels_a == 1)
    tpr_b = np.sum((true_labels_b == 1) & (pred_labels_b == 1)) / np.sum(true_labels_b == 1)

    # Equal opportunity ratio
    eo_ratio = min(tpr_a, tpr_b) / max(tpr_a, tpr_b)

    print(f"True Positive Rate (Group A): {tpr_a:.2%}")
    print(f"True Positive Rate (Group B): {tpr_b:.2%}")
    print(f"Equal Opportunity Ratio: {eo_ratio:.3f}")

    if eo_ratio >= 0.90:
        print("✅ Fair - similar accuracy across groups")
    elif eo_ratio >= 0.80:
        print("⚠️  Model performs worse for one group")
    else:
        print("❌ Significant performance gap - investigate training data imbalance")

    return eo_ratio

# Target: Equal opportunity ratio > 0.90
