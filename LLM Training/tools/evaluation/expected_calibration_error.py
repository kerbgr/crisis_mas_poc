"""
Extracted from LLM Training Methodology.
Auto-generated by extract_code_from_methodology.py
"""

import numpy as np

def expected_calibration_error(predictions, labels, confidences, n_bins=10):
    """
    Calculate ECE: How well does confidence match accuracy?

    Args:
        predictions: Model predictions (class labels)
        labels: Ground truth labels
        confidences: Model confidence scores (0.0-1.0)
        n_bins: Number of bins for calibration plot

    Returns:
        ece: Expected Calibration Error (0.0 = perfect, higher = worse)
    """
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]

    accuracies_in_bins = []
    confidences_in_bins = []
    proportions_in_bins = []

    ece = 0.0
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        # Find predictions in this confidence bin
        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)

        if in_bin.sum() > 0:
            # Calculate accuracy in this bin
            accuracy_in_bin = (predictions[in_bin] == labels[in_bin]).mean()
            avg_confidence_in_bin = confidences[in_bin].mean()
            proportion_in_bin = in_bin.sum() / len(predictions)

            # ECE is weighted absolute difference
            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * proportion_in_bin

            # Store for calibration plot
            accuracies_in_bins.append(accuracy_in_bin)
            confidences_in_bins.append(avg_confidence_in_bin)
            proportions_in_bins.append(proportion_in_bin)

    return ece, (accuracies_in_bins, confidences_in_bins, proportions_in_bins)

# Usage
ece, plot_data = expected_calibration_error(predictions, labels, confidences)
print(f"ECE: {ece:.3f}")

# Target: ECE < 0.1 (well-calibrated)
# Warning: ECE > 0.15 (poorly calibrated, unsafe)
