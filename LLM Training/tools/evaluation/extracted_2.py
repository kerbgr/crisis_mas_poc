"""
Extracted from LLM Training Methodology.
Auto-generated by extract_code_from_methodology.py
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained("./firefighter-llama3.1-8b")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3.1-8B-Instruct")

# Test set
test_texts = [
  "The IDLH for ammonia is 300 ppm...",
  "Wildfire suppression in Mediterranean climate...",
  # ... more domain texts
]

# Calculate perplexity
total_loss = 0
for text in test_texts:
    inputs = tokenizer(text, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
        total_loss += outputs.loss.item()

perplexity = torch.exp(torch.tensor(total_loss / len(test_texts)))
print(f"Perplexity: {perplexity:.2f}")
