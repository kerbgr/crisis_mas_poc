"""
Extracted from LLM Training Methodology.
Auto-generated by extract_code_from_methodology.py
"""

robustness_tests_typos = [
    {
        "input": "What is the DIHD for amonia?",  # IDLH → DIHD, ammonia → amonia
        "expected": "Model should still understand (autocorrect or contextual inference)",
        "acceptable_responses": ["300 ppm", "IDLH", "ammonia"]
    },
    {
        "input": "A wildifre is aproaching a vilage",
        "expected": "Model understands: wildfire, approaching, village",
        "acceptable_responses": ["evacuate", "fire", "safety"]
    }
]

def test_typo_robustness(model, tests):
    """Test if model can handle common typos."""
    robust_count = 0

    for test in tests:
        response = model.generate(test["input"])

        # Check if any acceptable response appears
        if any(keyword in response.lower() for keyword in test["acceptable_responses"]):
            robust_count += 1
        else:
            print(f"FAILED: {test['input']} → {response}")

    robustness_score = robust_count / len(tests)
    print(f"Typo Robustness: {robustness_score:.1%}")
    return robustness_score

# Target: >80% correct despite typos
