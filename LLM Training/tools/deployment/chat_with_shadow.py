"""
Extracted from LLM Training Methodology.
Auto-generated by extract_code_from_methodology.py
"""

import threading

@app.route("/v1/chat/completions", methods=["POST"])
def chat_with_shadow():
    data = request.json
    request_id = str(time.time())

    # Primary: Model A (production)
    start_time = time.time()
    response_a = model_a.generate(data["messages"])
    latency_a = time.time() - start_time

    # Shadow: Model B (run in background, don't wait)
    def shadow_inference():
        start_time = time.time()
        try:
            response_b = model_b.generate(data["messages"])
            latency_b = time.time() - start_time

            # Log for comparison
            log_shadow_comparison(
                request_id=request_id,
                input=data["messages"],
                response_a=response_a,
                response_b=response_b,
                latency_a=latency_a,
                latency_b=latency_b
            )
        except Exception as e:
            log_shadow_error(request_id, str(e))

    # Run shadow model in background thread
    threading.Thread(target=shadow_inference).start()

    # Return Model A response immediately
    return jsonify({"choices": [{"message": {"content": response_a}}]})

def log_shadow_comparison(request_id, input, response_a, response_b, latency_a, latency_b):
    """Compare shadow model output to production."""
    log_entry = {
        "request_id": request_id,
        "input": input,
        "model_a": {"response": response_a, "latency_ms": latency_a * 1000},
        "model_b": {"response": response_b, "latency_ms": latency_b * 1000},
        "outputs_match": response_a.strip() == response_b.strip()
    }

    with open("shadow_comparison.jsonl", "a") as f:
        f.write(json.dumps(log_entry) + "\n")
