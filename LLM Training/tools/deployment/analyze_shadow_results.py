"""
Extracted from LLM Training Methodology.
Auto-generated by extract_code_from_methodology.py
"""

# tools/analyze_shadow_results.py

import json

def analyze_shadow_logs(log_file="shadow_comparison.jsonl"):
    """Analyze shadow deployment results."""

    with open(log_file, "r") as f:
        logs = [json.loads(line) for line in f]

    total = len(logs)
    outputs_match = sum(1 for log in logs if log["outputs_match"])
    avg_latency_a = sum(log["model_a"]["latency_ms"] for log in logs) / total
    avg_latency_b = sum(log["model_b"]["latency_ms"] for log in logs) / total

    print(f"Shadow Deployment Results ({total} requests):")
    print(f"  Outputs match: {outputs_match}/{total} ({100*outputs_match/total:.1f}%)")
    print(f"  Model A latency: {avg_latency_a:.1f}ms")
    print(f"  Model B latency: {avg_latency_b:.1f}ms")
    print(f"  Latency improvement: {100*(avg_latency_a - avg_latency_b)/avg_latency_a:+.1f}%")

    # Sample disagreements
    disagreements = [log for log in logs if not log["outputs_match"]][:5]
    print(f"\nSample disagreements:")
    for log in disagreements:
        print(f"\nInput: {log['input']}")
        print(f"Model A: {log['model_a']['response'][:100]}...")
        print(f"Model B: {log['model_b']['response'][:100]}...")

analyze_shadow_logs()
