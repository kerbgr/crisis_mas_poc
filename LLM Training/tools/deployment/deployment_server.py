"""
Extracted from LLM Training Methodology.
Auto-generated by extract_code_from_methodology.py
"""

# tools/deployment_server.py

from flask import Flask, request, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

app = Flask(__name__)

# Load model at startup
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3.1-8B-Instruct")
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3.1-8B-Instruct",
    load_in_8bit=True,
    device_map="auto"
)
model = PeftModel.from_pretrained(base_model, "./lora/firefighter")

@app.route("/v1/chat/completions", methods=["POST"])
def chat():
    data = request.json
    messages = data["messages"]

    # Generate response
    input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")
    outputs = model.generate(
        input_ids,
        max_new_tokens=data.get("max_tokens", 512),
        temperature=data.get("temperature", 0.7),
        do_sample=True
    )

    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return jsonify({
        "choices": [{"message": {"role": "assistant", "content": response_text}}]
    })

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000)
