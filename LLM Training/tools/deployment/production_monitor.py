"""
Extracted from LLM Training Methodology.
Auto-generated by extract_code_from_methodology.py
"""

# tools/production_monitor.py

import time
import json
from collections import deque
from dataclasses import dataclass
from typing import Dict, List
import numpy as np

@dataclass
class ResponseMetrics:
    """Track metrics for each model response."""
    timestamp: float
    request_id: str
    latency_ms: float
    input_tokens: int
    output_tokens: int
    user_feedback: int  # -1 (thumbs down), 0 (no feedback), 1 (thumbs up)
    safety_flags: List[str]  # ["hallucination", "unsafe_recommendation", etc.]
    confidence_score: float  # Model's self-reported confidence

class ProductionMonitor:
    """Real-time monitoring for production LLM."""

    def __init__(self, window_size=1000):
        self.window_size = window_size
        self.recent_metrics = deque(maxlen=window_size)

        # Baseline metrics (from initial deployment)
        self.baseline = {
            "avg_latency_ms": 300.0,
            "thumbs_up_rate": 0.75,
            "safety_failure_rate": 0.02,
            "avg_confidence": 0.80
        }

        # Alert thresholds
        self.thresholds = {
            "latency_p95_ms": 800,  # Alert if p95 latency > 800ms
            "thumbs_down_rate": 0.20,  # Alert if >20% thumbs down
            "safety_failure_rate": 0.05,  # Alert if >5% safety failures
            "confidence_drop": 0.15  # Alert if confidence drops >15%
        }

    def log_response(self, metrics: ResponseMetrics):
        """Log a model response for monitoring."""
        self.recent_metrics.append(metrics)

        # Check for immediate issues
        self._check_alerts(metrics)

        # Periodic drift detection (every 100 requests)
        if len(self.recent_metrics) % 100 == 0:
            self._detect_drift()

    def _check_alerts(self, metrics: ResponseMetrics):
        """Check for immediate issues requiring alerts."""

        # Alert 1: High latency
        if metrics.latency_ms > self.thresholds["latency_p95_ms"]:
            self._send_alert(
                severity="warning",
                message=f"High latency detected: {metrics.latency_ms:.0f}ms",
                request_id=metrics.request_id
            )

        # Alert 2: Safety flags
        if len(metrics.safety_flags) > 0:
            self._send_alert(
                severity="critical",
                message=f"Safety issue: {metrics.safety_flags}",
                request_id=metrics.request_id
            )

        # Alert 3: Low confidence on critical query
        if metrics.confidence_score < 0.5:
            self._send_alert(
                severity="info",
                message=f"Low confidence response: {metrics.confidence_score:.2f}",
                request_id=metrics.request_id
            )

    def _detect_drift(self):
        """Detect performance drift over time."""

        if len(self.recent_metrics) < 100:
            return  # Not enough data

        current_metrics = self._calculate_current_metrics()

        # Compare to baseline
        issues = []

        # Check latency drift
        if current_metrics["avg_latency_ms"] > self.baseline["avg_latency_ms"] * 1.5:
            issues.append(f"Latency increased by {100*(current_metrics['avg_latency_ms']/self.baseline['avg_latency_ms']-1):.0f}%")

        # Check user satisfaction drift
        if current_metrics["thumbs_up_rate"] < self.baseline["thumbs_up_rate"] - 0.10:
            issues.append(f"User satisfaction dropped to {current_metrics['thumbs_up_rate']:.1%}")

        # Check safety drift
        if current_metrics["safety_failure_rate"] > self.baseline["safety_failure_rate"] * 2:
            issues.append(f"Safety failures increased to {current_metrics['safety_failure_rate']:.1%}")

        # Check confidence drift
        if current_metrics["avg_confidence"] < self.baseline["avg_confidence"] - self.thresholds["confidence_drop"]:
            issues.append(f"Model confidence dropped to {current_metrics['avg_confidence']:.2f}")

        if issues:
            self._send_alert(
                severity="warning",
                message="Performance drift detected:\n" + "\n".join(f"  - {issue}" for issue in issues)
            )

    def _calculate_current_metrics(self) -> Dict:
        """Calculate metrics over recent window."""

        recent = list(self.recent_metrics)[-self.window_size:]

        return {
            "avg_latency_ms": np.mean([m.latency_ms for m in recent]),
            "p95_latency_ms": np.percentile([m.latency_ms for m in recent], 95),
            "thumbs_up_rate": sum(1 for m in recent if m.user_feedback == 1) / len(recent),
            "thumbs_down_rate": sum(1 for m in recent if m.user_feedback == -1) / len(recent),
            "safety_failure_rate": sum(1 for m in recent if len(m.safety_flags) > 0) / len(recent),
            "avg_confidence": np.mean([m.confidence_score for m in recent])
        }

    def _send_alert(self, severity: str, message: str, request_id: str = None):
        """Send alert to monitoring system."""
        alert = {
            "timestamp": time.time(),
            "severity": severity,
            "message": message,
            "request_id": request_id
        }

        # Log to file
        with open("production_alerts.jsonl", "a") as f:
            f.write(json.dumps(alert) + "\n")

        # Send to monitoring service (Slack, PagerDuty, etc.)
        if severity == "critical":
            self._send_to_pagerduty(alert)
        else:
            self._send_to_slack(alert)

        print(f"[{severity.upper()}] {message}")

    def _send_to_slack(self, alert: Dict):
        """Send alert to Slack channel."""
        # Placeholder - integrate with your Slack webhook
        pass

    def _send_to_pagerduty(self, alert: Dict):
        """Send critical alert to PagerDuty."""
        # Placeholder - integrate with PagerDuty API
        pass

    def get_dashboard_metrics(self) -> Dict:
        """Get current metrics for monitoring dashboard."""

        if len(self.recent_metrics) == 0:
            return {}

        current = self._calculate_current_metrics()

        # Calculate trends (last 100 vs previous 100)
        if len(self.recent_metrics) >= 200:
            recent_100 = list(self.recent_metrics)[-100:]
            previous_100 = list(self.recent_metrics)[-200:-100]

            recent_latency = np.mean([m.latency_ms for m in recent_100])
            previous_latency = np.mean([m.latency_ms for m in previous_100])
            latency_trend = "↑" if recent_latency > previous_latency * 1.1 else "↓" if recent_latency < previous_latency * 0.9 else "→"

            recent_satisfaction = sum(1 for m in recent_100 if m.user_feedback == 1) / len(recent_100)
            previous_satisfaction = sum(1 for m in previous_100 if m.user_feedback == 1) / len(previous_100)
            satisfaction_trend = "↑" if recent_satisfaction > previous_satisfaction * 1.1 else "↓" if recent_satisfaction < previous_satisfaction * 0.9 else "→"
        else:
            latency_trend = "→"
            satisfaction_trend = "→"

        return {
            "current": current,
            "baseline": self.baseline,
            "trends": {
                "latency": latency_trend,
                "satisfaction": satisfaction_trend
            },
            "total_requests": len(self.recent_metrics)
        }

# Usage in production server
monitor = ProductionMonitor(window_size=1000)

@app.route("/v1/chat/completions", methods=["POST"])
def chat():
    data = request.json
    request_id = str(time.time())

    start_time = time.time()

    # Generate response
    response = model.generate(data["messages"])

    latency_ms = (time.time() - start_time) * 1000

    # Extract confidence (if model provides it)
    confidence = extract_confidence(response)

    # Detect safety issues
    safety_flags = detect_safety_issues(response)

    # Log metrics
    metrics = ResponseMetrics(
        timestamp=time.time(),
        request_id=request_id,
        latency_ms=latency_ms,
        input_tokens=len(tokenizer.encode(str(data["messages"]))),
        output_tokens=len(tokenizer.encode(response)),
        user_feedback=0,  # Updated later via feedback endpoint
        safety_flags=safety_flags,
        confidence_score=confidence
    )

    monitor.log_response(metrics)

    return jsonify({"choices": [{"message": {"content": response}}]})

# Feedback endpoint
@app.route("/feedback", methods=["POST"])
def feedback():
    """Collect user feedback (thumbs up/down)."""
    data = request.json
    request_id = data["request_id"]
    feedback_value = data["feedback"]  # 1 or -1

    # Update metrics for this request
    for metrics in monitor.recent_metrics:
        if metrics.request_id == request_id:
            metrics.user_feedback = feedback_value
            break

    return jsonify({"status": "feedback recorded"})
