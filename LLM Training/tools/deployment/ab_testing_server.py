"""
Extracted from LLM Training Methodology.
Auto-generated by extract_code_from_methodology.py
"""

# tools/ab_testing_server.py

import random
from flask import Flask, request, jsonify
import time
import json

app = Flask(__name__)

# Load both models
model_a = load_model("./models/firefighter-v1.0-production")  # Current
model_b = load_model("./models/firefighter-v1.1-candidate")   # New

# A/B test configuration
AB_TEST_CONFIG = {
    "model_a_weight": 0.8,  # 80% traffic to A (current model)
    "model_b_weight": 0.2,  # 20% traffic to B (new model)
    "enabled": True
}

# Metrics storage
metrics = {
    "model_a": {"requests": 0, "total_latency": 0, "errors": 0},
    "model_b": {"requests": 0, "total_latency": 0, "errors": 0}
}

def select_model():
    """Randomly select model based on weights."""
    if not AB_TEST_CONFIG["enabled"]:
        return model_a, "model_a"

    rand = random.random()
    if rand < AB_TEST_CONFIG["model_a_weight"]:
        return model_a, "model_a"
    else:
        return model_b, "model_b"

@app.route("/v1/chat/completions", methods=["POST"])
def chat():
    data = request.json
    request_id = str(time.time())

    # Select model for this request
    model, model_name = select_model()

    # Track metrics
    start_time = time.time()

    try:
        # Generate response
        response = model.generate(data["messages"])

        latency = time.time() - start_time

        # Log for analysis
        log_ab_test_result(
            request_id=request_id,
            model_name=model_name,
            input=data["messages"],
            output=response,
            latency=latency,
            error=None
        )

        # Update metrics
        metrics[model_name]["requests"] += 1
        metrics[model_name]["total_latency"] += latency

        return jsonify({"choices": [{"message": {"content": response}}]})

    except Exception as e:
        latency = time.time() - start_time

        # Log error
        log_ab_test_result(
            request_id=request_id,
            model_name=model_name,
            input=data["messages"],
            output=None,
            latency=latency,
            error=str(e)
        )

        metrics[model_name]["errors"] += 1

        # Fallback to model A on error
        if model_name == "model_b":
            response = model_a.generate(data["messages"])
            return jsonify({"choices": [{"message": {"content": response}}]})
        else:
            return jsonify({"error": str(e)}), 500

def log_ab_test_result(request_id, model_name, input, output, latency, error):
    """Log every A/B test result for later analysis."""
    log_entry = {
        "timestamp": time.time(),
        "request_id": request_id,
        "model": model_name,
        "input": input,
        "output": output,
        "latency_ms": latency * 1000,
        "error": error
    }

    with open("ab_test_logs.jsonl", "a") as f:
        f.write(json.dumps(log_entry) + "\n")

@app.route("/metrics", methods=["GET"])
def get_metrics():
    """View current A/B test metrics."""
    return jsonify({
        "model_a": {
            "requests": metrics["model_a"]["requests"],
            "avg_latency_ms": (metrics["model_a"]["total_latency"] / metrics["model_a"]["requests"] * 1000) if metrics["model_a"]["requests"] > 0 else 0,
            "error_rate": metrics["model_a"]["errors"] / metrics["model_a"]["requests"] if metrics["model_a"]["requests"] > 0 else 0
        },
        "model_b": {
            "requests": metrics["model_b"]["requests"],
            "avg_latency_ms": (metrics["model_b"]["total_latency"] / metrics["model_b"]["requests"] * 1000) if metrics["model_b"]["requests"] > 0 else 0,
            "error_rate": metrics["model_b"]["errors"] / metrics["model_b"]["requests"] if metrics["model_b"]["requests"] > 0 else 0
        }
    })

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000)
