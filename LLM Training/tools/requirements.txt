# LLM Training Requirements
# Python 3.10+ required

# Core training dependencies
torch>=2.0.0
transformers>=4.36.0
datasets>=2.14.0
accelerate>=0.25.0
peft>=0.7.0
trl>=0.7.4
bitsandbytes>=0.41.0

# Training frameworks (choose one or more)
axolotl @ git+https://github.com/OpenAccess-AI-Collective/axolotl.git  # Recommended
# llama-factory @ git+https://github.com/hiyouga/LLaMA-Factory.git
# unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git  # Faster training

# Data processing
pandas>=2.0.0
numpy>=1.24.0
jsonlines>=3.1.0
scikit-learn>=1.3.0

# Evaluation
nltk>=3.8.0
rouge-score>=0.1.2
bert-score>=0.3.13
sacrebleu>=2.3.1

# Model optimization
optimum>=1.16.0
auto-gptq>=0.6.0
llama-cpp-python>=0.2.0  # For GGUF quantization

# Monitoring and logging
wandb>=0.16.0  # Optional: for experiment tracking
tensorboard>=2.15.0
tqdm>=4.66.0

# Inference
vllm>=0.2.7  # Fast inference server
flash-attn>=2.5.0  # Flash Attention 2

# Development tools
jupyter>=1.0.0
ipython>=8.18.0
black>=23.12.0
flake8>=7.0.0

# Greek language support (optional)
spacy>=3.7.0
# python -m spacy download el_core_news_sm  # Greek model
