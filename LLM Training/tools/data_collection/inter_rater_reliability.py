"""
Extracted from LLM Training Methodology.
Auto-generated by extract_code_from_methodology.py
"""

# tools/inter_rater_reliability.py

from sklearn.metrics import cohen_kappa_score
import numpy as np

def calculate_cohens_kappa(expert1_ratings, expert2_ratings):
    """
    Calculate Cohen's Kappa for two experts.

    Args:
        expert1_ratings: List of ratings from expert 1 (e.g., [5, 4, 5, 3, 4])
        expert2_ratings: List of ratings from expert 2 (e.g., [5, 4, 4, 3, 5])

    Returns:
        kappa: Agreement score (0.0-1.0)
    """
    kappa = cohen_kappa_score(expert1_ratings, expert2_ratings)

    print(f"Cohen's Kappa: {kappa:.3f}")
    print(f"Interpretation: {interpret_kappa(kappa)}")

    return kappa

def interpret_kappa(kappa):
    """Interpret Cohen's Kappa score."""
    if kappa < 0:
        return "Poor (no agreement, worse than random)"
    elif kappa < 0.20:
        return "Slight (minimal agreement)"
    elif kappa < 0.40:
        return "Fair (low agreement)"
    elif kappa < 0.60:
        return "Moderate (acceptable agreement)"
    elif kappa < 0.80:
        return "Substantial (good agreement)"
    else:
        return "Almost Perfect (excellent agreement)"

# Example usage
expert1 = [5, 4, 5, 3, 4, 5, 2, 4, 5, 3]  # Quality ratings (1-5 scale)
expert2 = [5, 4, 4, 3, 5, 5, 2, 4, 4, 3]

kappa = calculate_cohens_kappa(expert1, expert2)

# Target: Kappa > 0.70 (substantial agreement)
if kappa < 0.40:
    print("⚠️ WARNING: Low agreement - review data collection process")
elif kappa < 0.70:
    print("⚠️ Moderate agreement - consider third expert review")
else:
    print("✓ Good agreement - data quality acceptable")
