"""
Extracted from LLM Training Methodology.
Auto-generated by extract_code_from_methodology.py
"""

def recommend_action(kappa):
    """Recommend action based on agreement level."""

    if kappa < 0.40:
        return """
⚠️ LOW AGREEMENT DETECTED (κ < 0.40)

Actions Required:
1. Review rating criteria with experts (are they using same standards?)
2. Provide calibration examples (show what 1-5 ratings mean)
3. Conduct group discussion to align understanding
4. Re-rate sample after calibration
5. If still low: simplify rating scale (binary good/bad instead of 1-5)
"""

    elif kappa < 0.70:
        return """
⚠️ MODERATE AGREEMENT (κ = 0.40-0.70)

Actions Recommended:
1. For disputed examples: Add third expert tie-breaker
2. Document disagreements for future training improvements
3. Consider averaging ratings instead of majority vote
4. Acceptable for training, but monitor model performance
"""

    else:
        return """
✅ GOOD AGREEMENT (κ > 0.70)

Actions:
1. Proceed with data collection
2. Continue using current rating guidelines
3. Periodically re-check agreement (every 500 examples)
"""

print(recommend_action(fleiss_k))
